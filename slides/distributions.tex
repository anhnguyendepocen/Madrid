\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage{palatino}
\usepackage{subfig}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage{minted}
\usepackage{multimedia}

\usetheme{Warsaw}
\usecolortheme{crane}

% www.sharelatex.com/learn/Beamer

\title{Probability Distributions}
\author{Brendon J. Brewer}
\institute{Department of Statistics\\
The University of Auckland}
\date{{\tt \color{blue} https://www.stat.auckland.ac.nz/\~{ }brewer/}}

\begin{document}

\frame{\titlepage}


% New slide
\begin{frame}
\frametitle{Probability Distributions}
Suppose a quantity $X$ might be 1, 2, 3, 4, or 5, and we assign probabilities of
$\frac{1}{5}$ to each of those possible values. There is some terminology:

\begin{itemize}
  \item<2-> $X$ is called a `random variable' (not often by me)
  \item<3-> $\{1, 2, 3, 4, 5\}$ is called the `sample space', `hypothesis space',
        or `parameter space'
  \item<4-> $\boldsymbol{p} = \left\{\frac{1}{5}, \frac{1}{5}, \frac{1}{5},
                \frac{1}{5}, \frac{1}{5}\right\}$
        is the `probability distribution' for
        $x$. In this case, it is a {\em discrete uniform} distribution.
\end{itemize}

\visible<5>{
The probability distribution is often written as $P(X=x) =$ (some function of $x$).}

\end{frame}



% New slide
\begin{frame}
\frametitle{Properties of discrete probability distributions}

\pause
Normalisation:\\ $\sum_x P(X=x) = 1$.\vspace{0.5em}

\pause
Expected value:\\$\mathds{E}(X) = \left<X\right> = \sum_x x P(X=x)$\vspace{0.5em}

\pause
Variance:\\$\textnormal{Var}(X) = \mathds{E}\left((X - \mathds{E}(X))^2\right) = \sum_x (x - \mathds{E}(X))^2 P(X=x)$\vspace{0.5em}

\pause
Standard deviation:\\$\textnormal{sd}(X) = \sqrt{\textnormal{Var}(X)}$

\pause
The expected value and sd describe the {\em center} and {\em width}
of the distribution respectively.

\end{frame}


% New slide
\begin{frame}
\frametitle{Shorthand notation}
$P(X=x)$ is cumbersome. $x$ is also just a dummy variable.

Common shorthand notation: 
Use $p(x)$ instead, equivocate between the quantity itself and the dummy
variable. E.g.:
\begin{align}
\mathds{E}(x) = \sum x p(x)
\end{align}

\end{frame}


% New slide
\begin{frame}[t, fragile]
\frametitle{Numerical handling}
Numerical handling of discrete probability distributions for a single
quantity: \vspace{2em}

\begin{minted}{python}
xs = np.arange(5, 21) # Grid of possibilities
ps = xs**2            # Not normalised
ps = ps/ps.sum()      # Normalise it
plt.bar(xs, ps)       # Plot it

# Expected value and variance
ex = np.sum(xs*ps) 
variance = np.sum(ps*(xs - ex)**2)
np.sum(ps[xs >= 10]) # P(x >= 10)
\end{minted}


\end{frame}




% New slide
\begin{frame}[t, fragile]
\frametitle{Common discrete distributions}

Here are some common discrete distributions:

\begin{itemize}
\item <2-> Discrete Uniform (which one is it?)
\item <3-> Binomial (how many successes out of $N$ quasi-identical trials?)
\item <4-> Poisson (how many occurrences of rare event?)
\end{itemize}

\end{frame}


% New slide
\begin{frame}[t, fragile]
\frametitle{Astronomy uses for Discrete Uniform}



\end{frame}


% New slide
\begin{frame}[t, fragile]
\frametitle{Astronomy uses for Binomial}

Suppose it is known (or hypothesised) that 30\% of stars of a particular type
exhibit a certain kind of oscillation signal. In a new sample of $N=100$
such stars, let $x$ be the number that have the oscillation.\vspace{1em}

Then $x \sim \textnormal{Binomial}(100, 0.3)$.

\vspace{2em}
{\tiny I set a probability equal to a frequency here. What implicit
 assumption am I making?}
\end{frame}



% New slide
\begin{frame}[t, fragile]
\frametitle{Astronomy uses for Poisson}


The intensity of an X-ray source is such that you would expect to detect
$\lambda$ photons per minute. Let $x$ be the actual number of photons you
observe in a minute.

\begin{align}
x | \lambda &\sim \textnormal{Poisson}(\lambda) \\
p(x | \lambda) &= \frac{\lambda^x e^{-\lambda}}{x!}
\end{align}

where $\lambda \geq 0$ and $x \in \{0, 1, 2, 3, ..., \}$.


\end{frame}


% New slide
\begin{frame}[t, fragile]
\frametitle{Three Poisson Distributions}

\begin{center}
  \includegraphics[width=0.7\textwidth]{poisson.pdf}
\end{center}

\end{frame}


% New slide
\begin{frame}[t, fragile]
\frametitle{Continuous distributions}

These are characterised by a continuous hypothesis space (e.g.
``all real numbers'') and a {\em probability density function} (PDF).
\vspace{1em}

For example, normal/gaussian distributions:

\begin{align}
X | \mu, \sigma &\sim \textnormal{Normal}(\mu, \sigma^2) \\
f_X(x) &= \frac{1}{\sigma \sqrt{2\pi}} \exp\left[-\frac{1}{2\sigma^2(x-\mu)^2}\right]
\end{align}

$f_X(x)$ is the full notation favoured by many statisticians.
You can also just write $f(x)$ or $p(x)$ (and not having any upper-case $X$).

\end{frame}


% New slide
\begin{frame}
\frametitle{Properties of continuous probability distributions}

Just like discrete, with integrals (implicitly over all $x$) replacing sums!

\pause
Normalisation:\\ $\int p(x) \, dx = 1$.\vspace{0.5em}

\pause
Expected value:\\$\mathds{E}(X) = \left<X\right> = \int x p(x) \, dx$\vspace{0.5em}

\pause
Variance:\\$\textnormal{Var}(X) = \mathds{E}\left((X - \mathds{E}(X))^2\right) = \int (x - \mathds{E}(X))^2 f(x) \, dx$\vspace{0.5em}

\pause
Standard deviation:\\$\textnormal{sd}(X) = \sqrt{\textnormal{Var}(X)}$

\pause
The expected value and sd describe the {\em center} and {\em width}
of the distribution respectively.

\end{frame}


% New slide
\begin{frame}[t, fragile]
\frametitle{Numerical handling}
Numerical handling of continuous probability distributions for a single
quantity: \vspace{2em}

\begin{minted}[fontsize=\scriptsize]{python}
xs = np.linspace(-10.0, 10.0, 10001)  # Grid of possibilities
ps = np.exp(-0.5*((xs - 2.0)/1.5)**2) # Not normalised
ps = ps/np.trapz(ps, x=xs) # Normalise it (integral, not sum)
plt.plot(xs, ps)           # Plot it

# Expected value and variance - integrals, not sums
ex = np.trapz(xs*ps, x=xs) 
variance = np.trapz(ps*(xs - ex)**2, x=xs)
np.trapz(ps*(xs >= 5.0), x=xs) # P(x >= 5)
\end{minted}
\end{frame}


% New slide
\begin{frame}[t, fragile]
\frametitle{Common continuous distributions}

Here are some common continuous distributions:

\begin{itemize}
\item <2-> Uniform (`lots of uncertainty')
\item <3-> Normal/gaussian (commonly used for noise)
\item <4-> $t$ (like a normal but with fatter tails)
\item <5-> Gamma (nice two-parameter family for a positive quantity)
\item <6-> Exponential (how long between subsequent events?)
\item <7-> Pareto (`power law')
\end{itemize}

\end{frame}



% New slide
\begin{frame}[t, fragile]
\frametitle{Astronomy uses for Uniform}

There is an asteroid somewhere in an image. Where?

\end{frame}


% New slide
\begin{frame}[t, fragile]
\frametitle{Astronomy uses for Uniform}

There is an asteroid somewhere in an image. Where?

\end{frame}




% New slide
\begin{frame}[t, fragile]
\frametitle{Higher dimensions}

Joint probability distributions can be created using the product rule.
e.g.,

\begin{align}
p(x, y) &= p(x) p(y | x)
\end{align}

The joint distribution allows you to calculate the probability of statements
about the {\em pair} $(x, y)$.

\begin{align}
P\big((x, y) \in R\big) &= \int_R p(x, y) \, dx \, dy
\end{align}



\end{frame}


% New slide
\begin{frame}[t, fragile]
\frametitle{Marginalisation}

If you have a joint distribution but only care about statements about one of
the quantities, you can find the {\em marginal distribution}:\vspace{2em}

\begin{align}
p(x) &= \int_{-\infty}^{\infty} p(x, y) \, dy
\end{align}



\end{frame}


% New slide
\begin{frame}[t, fragile]
\frametitle{Bayesian and frequentist uses}

Probability distributions are used in both Bayesian and frequentist senses.

\end{frame}



% -------------- New slide -------------
\begin{frame}
\frametitle{A frequency distribution...}

\centering
\includegraphics[scale=0.3]{height1.pdf}

\begin{eqnarray*}
\textnormal{Frac}\left(x \geq 1.8 | x \in [1.5, 2.0]\right)
&=& 
\frac{\textnormal{Frac}\left(x \geq 1.8, x \in [1.5, 2.0]\right)}
{\textnormal{Frac}\left(x \in [1.5, 2.0]\right)}\\
&=& \frac{\int_{1.8}^{2.0} f(x) \, dx}{\int_{1.5}^{2.0} f(x) \, dx}
\end{eqnarray*}

\end{frame}


% -------------- New slide -------------
\begin{frame}
\frametitle{A probability distribution...}

\centering
\includegraphics[scale=0.3]{height2.pdf}

\begin{eqnarray*}
\textnormal{Plaus}\left(x \geq 1.8 | x \in [1.5, 2.0]\right)
&=& 
\frac{\textnormal{Plaus}\left(x \geq 1.8, x \in [1.5, 2.0]\right)}
{\textnormal{Plaus}\left(x \in [1.5, 2.0]\right)}\\
&=& \frac{\int_{1.8}^{2.0} f(x) \, dx}{\int_{1.5}^{2.0} f(x) \, dx}
\end{eqnarray*}

\end{frame}


% -------------- New slide -------------
\begin{frame}
\frametitle{The biggest source of confusion in statistics}

\begin{alertblock}{Important!}
The biggest source of confusion in statistics is the failure to
distinguish between {\bf frequency distributions} which describe
{\em populations}, and {\bf probability distributions} which describe
{\em uncertainty about a single quantity\footnote{Could be a single
non-scalar quantity, such as $(3.2, 1.7)$.}}.
\end{alertblock}

\end{frame}



\end{document}


